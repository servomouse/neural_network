#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include "neuron.h"
#include "micronet.h"
#include "utils.h"

#define sizeof_arr(_x) sizeof(_x)/sizeof(_x[0])

// Neuron types:
// 0 - linear
// 1 - polynomial

uint32_t poly_neurons[] = {
    // idx  num_inputs  type indices
       8,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
       9,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      10,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      11,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      12,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      13,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      14,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      15,   8,          1,   0, 1, 2, 3, 4, 5, 6, 7,
      16,   8,          1,   8, 9, 10, 11, 12, 13, 14, 15
};

micronet_map_t poly_micronet_map = {
    .num_inputs = 8,
    .num_neurons = 9,
    .net_size = 17,
    .neurons = poly_neurons,
    .num_outputs = 1,
    .output_indices = {16},
};

double poly_coeffs[9][256] = {  // Plus offset
    {0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768},
    {-0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576},
    {-0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384},
    {-0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192},
    {-0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768},
    {0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576},
    {0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384},
    {0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768,
     0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192},
    {0.006, 0.012, 0.018, 0.024, 0.030, 0.036, 0.042, 0.048, 0.054, 0.060, 0.066, 0.072, 0.078, 0.084, 0.090, 0.096,
     0.102, 0.108, 0.114, 0.120, 0.126, 0.132, 0.138, 0.144, 0.150, 0.156, 0.162, 0.168, 0.174, 0.180, 0.186, 0.192,
     0.198, 0.204, 0.210, 0.216, 0.222, 0.228, 0.234, 0.240, 0.246, 0.252, 0.258, 0.264, 0.270, 0.276, 0.282, 0.288,
     0.294, 0.300, 0.306, 0.312, 0.318, 0.324, 0.330, 0.336, 0.342, 0.348, 0.354, 0.360, 0.366, 0.372, 0.378, 0.384,
     0.390, 0.396, 0.402, 0.408, 0.414, 0.420, 0.426, 0.432, 0.438, 0.444, 0.450, 0.456, 0.462, 0.468, 0.474, 0.480,
     0.486, 0.492, 0.498, 0.504, 0.510, 0.516, 0.522, 0.528, 0.534, 0.540, 0.546, 0.552, 0.558, 0.564, 0.570, 0.576,
     0.582, 0.588, 0.594, 0.600, 0.606, 0.612, 0.618, 0.624, 0.630, 0.636, 0.642, 0.648, 0.654, 0.660, 0.666, 0.672,
     0.678, 0.684, 0.690, 0.696, 0.702, 0.708, 0.714, 0.720, 0.726, 0.732, 0.738, 0.744, 0.750, 0.756, 0.762, 0.768,
     -0.006, -0.012, -0.018, -0.024, -0.030, -0.036, -0.042, -0.048, -0.054, -0.060, -0.066, -0.072, -0.078, -0.084, -0.090, -0.096,
     -0.102, -0.108, -0.114, -0.120, -0.126, -0.132, -0.138, -0.144, -0.150, -0.156, -0.162, -0.168, -0.174, -0.180, -0.186, -0.192,
     -0.198, -0.204, -0.210, -0.216, -0.222, -0.228, -0.234, -0.240, -0.246, -0.252, -0.258, -0.264, -0.270, -0.276, -0.282, -0.288,
     -0.294, -0.300, -0.306, -0.312, -0.318, -0.324, -0.330, -0.336, -0.342, -0.348, -0.354, -0.360, -0.366, -0.372, -0.378, -0.384,
     -0.390, -0.396, -0.402, -0.408, -0.414, -0.420, -0.426, -0.432, -0.438, -0.444, -0.450, -0.456, -0.462, -0.468, -0.474, -0.480,
     -0.486, -0.492, -0.498, -0.504, -0.510, -0.516, -0.522, -0.528, -0.534, -0.540, -0.546, -0.552, -0.558, -0.564, -0.570, -0.576,
     -0.582, -0.588, -0.594, -0.600, -0.606, -0.612, -0.618, -0.624, -0.630, -0.636, -0.642, -0.648, -0.654, -0.660, -0.666, -0.672,
     -0.678, -0.684, -0.690, -0.696, -0.702, -0.708, -0.714, -0.720, -0.726, -0.732, -0.738, -0.744, -0.750, -0.756, -0.762, -0.768},
};

typedef struct {
    double inputs[8];
    double outputs[1];
} dataset_entry_t;

dataset_entry_t poly_dataset[] = {
    {.inputs = {0.529770, 0.513530, -0.816710, 0.782040, -0.253210, 0.815240, 0.986080, -0.448590},     .outputs = {-0.002814}},
    {.inputs = {0.162020, 0.275190, -0.110570, 0.790030, -0.955080, -0.149450, 0.400800, -0.187050},    .outputs = {-0.043915}},
    {.inputs = {0.395550, -0.124000, 0.152010, 0.901060, -0.265300, -0.443340, -0.574880, -0.094390},   .outputs = {-0.057001}},
    {.inputs = {0.088470, 0.323890, -0.395860, -0.970270, 0.276890, 0.994020, -0.623890, -0.098360},    .outputs = {-0.062746}},
    {.inputs = {0.385110, -0.733820, 0.835140, 0.498580, 0.377300, -0.804740, -0.805480, -0.609420},    .outputs = {-0.074575}},
    {.inputs = {0.067480, 0.585740, 0.661790, -0.991090, 0.837890, -0.050140, 0.187780, -0.357770},     .outputs = {-0.188901}},
    {.inputs = {-0.247110, 0.635490, 0.105380, 0.166300, -0.898310, 0.349960, -0.785940, -0.522200},    .outputs = {-0.235908}},
    {.inputs = {0.150490, 0.703360, 0.302160, 0.826410, -0.986630, -0.991700, 0.850890, 0.355940},      .outputs = {-0.296192}},

    {.inputs = {0.448100, 0.749080, 0.715200, -0.158730, -0.506030, 0.129670, 0.381570, -0.672230},     .outputs = {0.000000}},
    {.inputs = {0.818840, -0.917290, 0.409590, -0.427720, -0.842590, 0.673270, -0.161530, 0.626090},    .outputs = {0.005922}},
    {.inputs = {0.833670, -0.612780, 0.191200, 0.915160, 0.745170, 0.022130, 0.572740, 0.728570},       .outputs = {0.011651}},
    {.inputs = {0.981750, 0.629810, -0.244970, -0.705010, -0.444320, 0.248210, 0.398420, 0.310280},     .outputs = {0.047309}},
    {.inputs = {-0.585680, 0.335800, -0.007420, -0.585620, 0.006260, 0.556020, 0.238620, -0.727590},    .outputs = {0.052585}},
    {.inputs = {-0.255170, 0.513290, 0.668810, 0.611930, -0.822690, 0.424600, 0.468490, 0.131570},      .outputs = {0.089573}},
    {.inputs = {-0.153660, 0.639390, -0.328650, 0.442920, -0.775750, 0.497360, 0.110870, -0.880670},    .outputs = {0.090832}},
    {.inputs = {-0.168310, -0.703480, -0.427720, 0.987060, 0.258100, 0.175820, -0.599050, -0.722530},   .outputs = {0.116900}},
};

static double get_error(micro_network_t *net, dataset_entry_t *dataset, size_t dataset_size, uint32_t num_outputs) {
    // Works only with single output networks
    double error = 0;
    for(size_t i=0; i<dataset_size; i++) {
        double *outputs = micronet_get_output(net, dataset[i].inputs);
        double e = 0.0;
        for(uint32_t j=0; j<num_outputs; j++) {
            double delta = dataset[i].outputs[j] - outputs[j];
            e += delta * delta;
        }

        printf("Desired outputs: [");
        for(uint32_t j=0; j<num_outputs-1; j++) {
            printf("%f, ", dataset[i].outputs[j]);
        }
        printf("%f], ", dataset[i].outputs[num_outputs-1]);

        printf("real outputs: [");
        for(uint32_t j=0; j<num_outputs-1; j++) {
            printf("%f, ", outputs[j]);
        }
        printf("%f];\n", outputs[num_outputs-1]);

        error += e;
    }
    return error / dataset_size;
}

static void train_net(micro_network_t *target_net, micro_network_t *c_net, micro_network_t *f_net, dataset_entry_t *dataset, size_t dataset_size, uint32_t num_outputs) {
    // Works only with single output networks
    // double error = 0;
    double *errors = calloc(num_outputs, sizeof(double));
    for(size_t i=0; i<dataset_size; i++) {
        double *outputs = micronet_get_output(target_net, dataset[i].inputs);
        double e = 0.0;
        for(uint32_t j=0; j<num_outputs; j++) {
            double delta = dataset[i].outputs[j] - outputs[j];
            errors[j] = delta * delta;
            e += errors[j];
        }
        micronet_set_global_error(target_net, e/num_outputs, errors);

        
        // double new_error = get_error(&target_net, linear_dataset, sizeof_arr(linear_dataset), 1);
        // double delta = new_error - init_error;
        // double e_val = delta * delta;
        // // #error "Update f_micronet (5 inputs and 2 outputs)"
        micronet_update_feedbacks(target_net, f_net);
        micronet_update_coeffs(target_net, c_net);

        // printf("Desired outputs: [");
        // for(uint32_t j=0; j<num_outputs-1; j++) {
        //     printf("%f, ", dataset[i].outputs[j]);
        // }
        // printf("%f], ", dataset[i].outputs[num_outputs-1]);

        // printf("real outputs: [");
        // for(uint32_t j=0; j<num_outputs-1; j++) {
        //     printf("%f, ", outputs[j]);
        // }
        // printf("%f];\n", outputs[num_outputs-1]);

        // error += e;
    }
}

/*
- global error
- feedback error
- neuron output
- coefficient value
- input value
- c_net stash
- f_net stash
- global stash
*/

uint32_t smart_neurons[] = {
    // idx  num_inputs  type indices
       5,   5,          1,   0, 1, 2, 3, 4,
       6,   5,          1,   0, 1, 2, 3, 4,
       7,   5,          1,   0, 1, 2, 3, 4,
       8,   5,          1,   0, 1, 2, 3, 4,
       9,   5,          1,   0, 1, 2, 3, 4,

      10,   5,          1,   5, 6, 7, 8, 9,
      11,   5,          1,   5, 6, 7, 8, 9,
};

micronet_map_t smart_micronet_map = {
    .num_inputs = 5,
    .num_neurons = 7,
    .net_size = 12,
    .neurons = smart_neurons,
    .num_outputs = 2,
    .output_indices = {10, 11},
};

void init_coeffs(micro_network_t *net) {
    for(uint32_t i=0; i<9; i++) {
        for(uint32_t j=0; j<256; j++) {
            neuron_set_coeff(&net->neurons[i], j, poly_coeffs[i][j]);
        }
    }
}

// static double get_delta(micro_network_t *target_net, micro_network_t *c_net, micro_network_t *f_net) {
//     double init_error = get_error(target_net, linear_dataset, sizeof_arr(linear_dataset), 1);
//     micronet_clear_feedbacks(target_net);
//     train_net(target_net, c_net, f_net, linear_dataset, sizeof_arr(linear_dataset), 1);

//     double final_error = get_error(target_net, linear_dataset, sizeof_arr(linear_dataset), 1);
//     if(init_error == 0.0) {
//         return 0;
//     }
//     double delta;
//     if(init_error > final_error) {
//         delta = (init_error - final_error) / init_error;
//     } else {
//         delta = -1 * (final_error - init_error) / final_error;
//     }
//     // printf("init_error: %f, final_error: %f, delta: %f\n", init_error, final_error, delta);
//     return delta;
// }

// static double get_average_delta(micro_network_t *target_net, micro_network_t *c_net, micro_network_t *f_net, double coeff_inc) {
//     double init_average_delta = 0.0;
//     uint32_t average_delta_counter = 0;

//     for(uint32_t n=0; n<5; n++) {
//         for(uint32_t c=0; c<5; c++) {
//             init_coeffs(target_net);
//             neuron_set_coeff(&target_net->neurons[n], c, coeff_inc);

//             init_average_delta += get_delta(target_net, c_net, f_net);
//             average_delta_counter++;
//         }
//     }

//     return init_average_delta / average_delta_counter;
// }

int main(void) {
    srand(time(NULL));
    micro_network_t target_net = {0};
    micronet_init(&target_net, &poly_micronet_map);
    init_coeffs(&target_net);
    // micronet_print_coeffs(&target_net);
    // printf("Target microNet initialised!\n");
    // micro_network_t c_micronet = {0};
    // micronet_init(&c_micronet, &smart_micronet_map);
    // printf("C microNet initialised!\n");
    // micro_network_t f_micronet = {0};
    // micronet_init(&f_micronet, &smart_micronet_map);
    // printf("F microNet initialised!\n");

    // char *c_net_path = BCKP_DIR_PATH "/poly/c_unet";
    // char *f_net_path = BCKP_DIR_PATH "/poly/f_unet";
    // micronet_restore(&c_micronet, c_net_path);
    // micronet_restore(&f_micronet, f_net_path);

    // // Dataset generator
    // for(uint32_t i=0; i<16; i++) {
    //     double inputs[8] = {
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //         random_double(-1, 1),
    //     };
    //     double *outputs = micronet_get_output(&target_net, inputs);
    //     printf("{.inputs = {%f, %f, %f, %f, %f, %f, %f, %f}, .outputs = {%f}},\n",
    //                                                                     inputs[0],
    //                                                                     inputs[1],
    //                                                                     inputs[2],
    //                                                                     inputs[3],
    //                                                                     inputs[4],
    //                                                                     inputs[5],
    //                                                                     inputs[6],
    //                                                                     inputs[7],
    //                                                                     outputs[0]);
    // }
    // return 0;

    double init_error = get_error(&target_net, poly_dataset, sizeof_arr(poly_dataset), 1);
    printf("Init error: %f\n", init_error);
    return 0;

    // char *n_path = concat_strings(BCKP_DIR_PATH, "/target_unet");
    // micronet_save(&target_net, n_path);
    
    // for(uint32_t i=0; i<5; i++) {
    //     for(uint32_t j=0; j<5; j++) {
    //         neuron_set_coeff(&target_net.neurons[i], j, 0.1);
    //     }
    // }
    // init_error = get_error(&target_net, linear_dataset, sizeof_arr(linear_dataset), 1);
    // printf("Error after save: %f\n", init_error);
    // micronet_restore(&target_net, n_path);
    // init_error = get_error(&target_net, linear_dataset, sizeof_arr(linear_dataset), 1);
    // printf("Error after restore: %f\n", init_error);
    // return 0;

    // double delta_sum = 0;
    // uint32_t delta_counter = 0;
    // uint32_t pos_delta_counter = 0;
    // uint32_t pos_new_delta_counter = 0;
    // uint32_t delta_improved_counter = 0;

    // for(uint32_t i=0; i<10000; i++) {
    //     // printf("Round %d:\n", i);

    //     // uint32_t coords[2] = {
    //     //     random_int(0, 5),
    //     //     random_int(0, 5)
    //     // };
    //     double coeff_inc = random_double(-0.5, 0.5);

    //     double init_average_delta = get_average_delta(&target_net, &c_micronet, &f_micronet, coeff_inc);

    //     delta_sum += init_average_delta;
    //     delta_counter ++;
    //     if(init_average_delta > 0) {
    //         pos_delta_counter ++;
    //     }

    //     micronet_mutate(&c_micronet);
    //     micronet_mutate(&f_micronet);

    //     double after_average_delta = get_average_delta(&target_net, &c_micronet, &f_micronet, coeff_inc);

    //     if(after_average_delta < init_average_delta) {
    //         micronet_rollback(&f_micronet);
    //         micronet_rollback(&c_micronet);
    //     }
    //     if(after_average_delta > init_average_delta) {
    //         delta_improved_counter ++;
    //     }
    //     if(i%1000 == 0) {
    //         printf("Init delta: %f, new_delta: %f, average delta: %f\n", init_average_delta, after_average_delta, delta_sum/delta_counter);
    //         fflush(stdout);
    //         if(i > 0) {
    //             micronet_save(&c_micronet, c_net_path);
    //             micronet_save(&f_micronet, f_net_path);
    //         }
    //     }
    //     if(after_average_delta > 0) {
    //         pos_new_delta_counter ++;
    //     }
    // }
    // // double current_error = get_error(&target_net, linear_dataset, sizeof_arr(linear_dataset), 1);
    // // printf("Current error: %f\n", current_error);
    // printf("Average_delta: %f, delta_counter = %d\n", delta_sum/delta_counter, delta_counter);
    // printf("Positive delta counter: %d\n", pos_delta_counter);
    // printf("Positive new delta counter: %d\n", pos_new_delta_counter);
    // printf("Improved delta counter: %d\n", delta_improved_counter);
    // printf("c_micronet coeffs:\n");
    // micronet_print_coeffs(&c_micronet);
    // printf("f_micronet coeffs:\n");
    // micronet_print_coeffs(&f_micronet);

    // // remove_folders(BCKP_DIR_PATH);
    // micronet_save(&c_micronet, c_net_path);
    // micronet_save(&f_micronet, f_net_path);
    return EXIT_SUCCESS;
}